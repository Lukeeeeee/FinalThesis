\documentclass{standalone}
% preamble: usepackage, etc.
\begin{document}

\chapter{全文总结与展望}
\section{全文总结}
在该文中，我们设计并实现了基于强化学习的路径规划算法。我们首先引入了路径规划问题的定义和相关扩展，然后介绍了基于搜索的传统方法，并简单分析了其局限性。随后我们介绍了强化学习的相关背景，例如马尔科夫决策过程，智能体建模方法，最后详细给出了强化学习的 Q-learning 算法。在五章，我们给出了我们的算法，包括如何将路径规划问题转化为一个标准的强化学习环境，然后对要解决的三个场景分别提出了基于 Q-learning 的算法，给出了相关的定义和算法流程。在第六章，我们给出了实验的实现细节，包括了代码设计的核心思路和用到的设计模式。然后给出实验参数设置和实验结果，通过实验结果可以看出我们的算法很好的解决了相应的场景。\par
通过模型设计和实验结果的展示，可以看出，基于强化学习的路径规划算法相对传统的基于图上的搜索方法具有较大的优势。其强大的灵活性可以适应多种的场景，我们只需要调整相应的马尔科夫决策过程的形式，特别是奖赏函数的设计，即可完成对不同场景的路径规划算法设计，而传统的方法对不同的场景难以提出一个统一算法模型。
\section{后续工作展望}
在我们的工作中，我们的算法解决了小规模地图下的多个场景下的路径规划问题，但在现实路径规划系统中，问题的规模远远大于我们的实验设计，因此如何将我们的算法迁移到大规模场景下是未来的研究方向之一，这首先将影响到我们算法的设计，基于查表法的 Q-leanring 难以处理如此大规模的数据，因此可以在接下来的研究中通过基于函数近似的方法，利用神经网络等方法，例如 DQN 等近些年提出的深度强化学习算法解决该问题。结合强大的计算能力和大规模的神经网络，这类方法能够处理更加复杂的路径规划问题\par
第二的方向为，如何设计一个基于强化学习的通用的路径规划算法，在我们的工作中，虽然解决了部分场景，算法并不属于同一个框架，在部分设计上存在不同的地方，虽然相比传统方法已经具有很高的灵活性，但也在一定程度上限制了强化学习方法的进一步应用，因此在未来的研究中，可以关注如何设计更加灵活的状态，行为和奖赏函数以使得模型的泛化能力提高。\par
第三，如何设计一个离线算法也是未来的研究方向之一。我们的模型现在只能给出在线的规划算法，但常见的路径规划系统往往具有离线规划的能力，即单次询问即可给出完整的规划结果，所以如果把强化学习实际应用到现实系统中，如何实现离线规划也是重要的研究方向。\par
最后，在基于环境交互进行学习的同时，如何利用现有的行驶数据进行学习也是未来的重要的研究方向，这也不仅仅是该问题上的发展方向，也是强化学习该领域的一个重要方向。如果能很好的利用现实世界采集的真实数据，会大大降低模拟环境的误差带来的问题，提升模型的效果。
\end{document}